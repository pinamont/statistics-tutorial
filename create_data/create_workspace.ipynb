{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47b21e24",
   "metadata": {},
   "source": [
    "# Workspace creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd2818d",
   "metadata": {},
   "source": [
    "_Valerio Ippolito - INFN Sezione di Roma_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c4576c",
   "metadata": {},
   "source": [
    "In this tutorial, we will create a simple example workspace based on existing histogram input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9814b7a5",
   "metadata": {},
   "source": [
    "We first import ROOT (we'll be using it interactively via pyROOT) and glob, a tool to conveniently loop over the content of a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc118d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f988d9",
   "metadata": {},
   "source": [
    "Let's identify the available files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb409ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('../data/*root')\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40fbbed",
   "metadata": {},
   "source": [
    "The structure of the file mimicks that of the histogram file created (\"backed up\") by workspace creation code. Histograms are named in an unique way, and stored in subfolders (of other subfolders), depending on which region (= statistically independent channel), sample (= physics process) and variation (= nominal, various systematic sources) they belong to.\n",
    "\n",
    "Let's define a recursive function to unpack in an unbiased way this structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03386567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_file(f, spacing=''):\n",
    "    for k in f.GetListOfKeys():        \n",
    "        if k.GetClassName() == 'TDirectoryFile':\n",
    "            print(f'{spacing} {k.GetName()}')\n",
    "            list_file(k.ReadObj(), spacing=f'{spacing}\\t')\n",
    "        else: # we found a histo or similar\n",
    "            print(f'{spacing} {k.GetName()} ({k.GetClassName()})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba5f4b7",
   "metadata": {},
   "source": [
    "... and another function to help us interpret the (long!) output. Here we expect a three-level structure:\n",
    "1. first level is the region folder\n",
    "2. second level is the sample name\n",
    "3. third level is the variation name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97fb00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_samples(f):\n",
    "    regions = set()\n",
    "    samples = set()\n",
    "    systs = set()\n",
    "    for k in f.GetListOfKeys():\n",
    "        if k.GetClassName() == 'TDirectoryFile':\n",
    "            regions.add(k.GetName())\n",
    "        for k2 in k.ReadObj().GetListOfKeys():\n",
    "            if k2.GetClassName() == 'TDirectoryFile':\n",
    "                samples.add(k2.GetName())\n",
    "                for k3 in k2.ReadObj().GetListOfKeys():\n",
    "                    if k3.GetClassName() == 'TDirectoryFile':\n",
    "                        systs.add(k3.GetName())\n",
    "    return regions, samples, systs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeab8e7",
   "metadata": {},
   "source": [
    "With these bullets in our gun, let's print out the content of the input files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c004d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = {} # one per region\n",
    "samples = {}\n",
    "systs = {}\n",
    "\n",
    "for fname in files:\n",
    "    print(f'\\nContent of file {fname}:')\n",
    "    f = ROOT.TFile(fname)\n",
    "    \n",
    "    list_file(f)\n",
    "    reg_, sam_, sys_ = find_samples(f)\n",
    "    assert(len(reg_) == 1) # only one region per file\n",
    "    fnames[list(reg_)[0]] = fname\n",
    "    samples[list(reg_)[0]] = sam_\n",
    "    systs[list(reg_)[0]] = sys_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107ad4fd",
   "metadata": {},
   "source": [
    "A more convenient way to look at this output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bb71b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in samples.keys():\n",
    "    print(f'\\nregion: {region}')\n",
    "    print(f'samples: {samples[region]}')\n",
    "    print(f'available variations: {systs[region]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86aebc1",
   "metadata": {},
   "source": [
    "Now that we understood what's available in the file, let's decide ourselves which subset of this info we will use to build our statistical analysis (our likelihood model).\n",
    "We'll decide:\n",
    "- which regions we want to consider (we may start with one of them and then extend to all of them)\n",
    "- which set of histograms we want to take (the suffix `_regBin` is used for the final histograms, while there are other without this suffix we will ignore)\n",
    "- how the data are called (we have a single set of data histograms, which contain the name `Data` - convenient, isn't it?)\n",
    "- what's the name of the nominal variation (i.e. what's the folder containing the nominal histograms - surprisingly, we had called it `nominal` when creating the histograms)\n",
    "- which samples we want to consider (TODO: a lists which so far isn't used anywhere, as we don't loop anymore on samples for educational reasons)\n",
    "- which systematics we want to consider (TODO: remove, as that's a superset and there are also overall systs involved for some reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f527d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = ['ljets_HThad_5j3b', 'ljets_Mbb_ge6jge4b']\n",
    "k_suffix = '_regBin'\n",
    "k_data = 'Data' # key for data\n",
    "k_nominal = 'nominal' # key for nominal\n",
    "samples = ['ttbar', 'singleTop', 'ttH']\n",
    "systs = ['nominal', 'ttHXsec', 'BTag_B_NP1', 'BTag_B_NP2', 'BTag_B_NP3', 'ttXsec', 'JES_Scenario1_NP1', 'JES_Scenario1_NP2', 'BTag_C_NP1', 'tt_Shower', 'Lumi', 'stXsec', 'JER']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccf71e5",
   "metadata": {},
   "source": [
    "HistFactory is so nice with us that we are allowed to make many tests, provided we pay the price of deciding how to name them (and the corresponding outputs). File names will always contain some nickname (to which we conveniently add the prefix `ATLASIT`), followed by names depicting which channel or combination of channels these files refer to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0076fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nick = 'prova'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f151d09",
   "metadata": {},
   "source": [
    "The goal of this notebook is to create a workspace, which means:\n",
    "- a file containing the actual ROOT workspace, one per \"configuration\" of the likelihood model (single-channel, all-channels)\n",
    "- a file containing the specification of how we built such workspace from the input ROOT files, in XML format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb025edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../ws\n",
    "!mkdir -p ../xml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5d5b47",
   "metadata": {},
   "source": [
    "Our likelihood model, and the meaning we give to it, is stored within a _measurement_ - an HistFactory concept which needs to know:\n",
    "- how we want to nickname it\n",
    "- where output files should be stored\n",
    "- what's the parameter of interest of this measurement\n",
    "- what are the parameters to be considered as a constant, if any - we typically include the default luminosity nuisance parameter created by HistFactory, called `Lumi`, within this \"blacklist\"\n",
    "- what are the default settings of the default luminosity parameter, used by HistFactory whenever you specify that a channel should be normalized by luminosity (see `SetNormalizeByTheory`)\n",
    "\n",
    "We are also nice people who like to decouple logical steps, so we ask HistFactory to kindly not do anything else than exporting the workspace into a ROOT file (i.e. please HistFactory do not perform any statistical analysis without our consent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e31527",
   "metadata": {},
   "outputs": [],
   "source": [
    "meas = ROOT.RooStats.HistFactory.Measurement(f'ATLASIT_{nick}', f'ATLASIT_{nick}')\n",
    "meas.SetOutputFilePrefix(f'../ws/ATLASIT_{nick}')\n",
    "\n",
    "meas.SetPOI('mu_ttH')\n",
    "\n",
    "meas.AddConstantParam('Lumi')\n",
    "meas.SetLumi(1.0)\n",
    "meas.SetLumiRelErr(0.0)\n",
    "meas.SetExportOnly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8365113d",
   "metadata": {},
   "source": [
    "Shortly speaking, systematics are divided in:\n",
    "- shape uncertainties, for which you normally provide a shape variation (i.e. another histogram which has the same binning but possibly different bin contents than the nominal one - for each region, channel and sample this systematic uncertainty applies to)\n",
    "- normalisation-only uncertainties, which are provided in a simple form as \"-1sigma\" and \"+1sigma\" variations - where the meaning of the sign is given by you, the analyser, and not by any mathematical constraint (i.e. +1sigma variations may also reduce the normalisation of a histogram with respect to its nominal value).\n",
    "\n",
    "Systematics are considered as correlated (i.e. they are represented by a single nuisance parameter) if they share the same name.\n",
    "\n",
    "Let's address the boring task of writing down our normalisation-only uncertainties: we follow the totally reasonable practice of indicating first the -1sigma and then the +1sigma effect of each nuisance parameter on the normalisation of each histogram (and we need to be careful in remembering this order for the rest of our code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1520f6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_systs = {\n",
    "    'ljets_HThad_5j3b': {\n",
    "        'ttH': {\n",
    "            'lumi':   (0.95, 1.05),\n",
    "            'ttXsec': (0.90, 1.10),\n",
    "            'JES_Scenario1_NP1': (1.17705, 0.822951),\n",
    "        },\n",
    "        'ttbar': {\n",
    "            'lumi':   (0.95, 1.05),\n",
    "            'ttXsec': (0.94, 1.06),\n",
    "            'JES_Scenario1_NP1': (0.926422, 1.07358),\n",
    "        },\n",
    "        'singleTop': {\n",
    "            'lumi':   (0.95, 1.05),\n",
    "            'stXsec': (0.95, 1.05),\n",
    "            'JES_Scenario1_NP1': (0.892136, 1.0786),\n",
    "        },\n",
    "    },\n",
    "     'ljets_Mbb_ge6jge4b': {\n",
    "        'ttH': {\n",
    "            'lumi':   (0.95, 1.05),\n",
    "            'ttXsec': (0.90, 1.10),\n",
    "            'JES_Scenario1_NP1': (0.946709, 1.05329),\n",
    "        },\n",
    "        'ttbar': {\n",
    "            'lumi':   (0.95, 1.05),\n",
    "            'ttXsec': (0.94, 1.06),\n",
    "            'JES_Scenario1_NP1': (0.925296, 1.0747),\n",
    "        },\n",
    "        'singleTop': {\n",
    "            'lumi':   (0.95, 1.05),\n",
    "            'stXsec': (0.95, 1.05),\n",
    "            'JES_Scenario1_NP1': (0.898541, 1.10146),\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7251f3b",
   "metadata": {},
   "source": [
    "We then follow this logic:\n",
    "1. we first create a channel (corresponding to some set of statistically-independent data)\n",
    "2. we tell HistFactory where (meaning: in which file, under which subdirectory path and more specifically in which histogram) to find the data for this channel\n",
    "3. we may indulge in specifying how uncertainties related to the limited MC statistics in signal/background histograms should be dealt with, in this channel\n",
    "4. we then add the samples which contribute to this channel, specifying where to find their nominal histograms, and which normalisation-only (`AddOverallSys`) and also-shape uncertainties (`AddHistoSys`) should be considered (keeping in mind that variations of any kind which share the same name are correlated)\n",
    "5. we also add free parameters to fit for determining the normalisation of our signal (and sometimes background) samples\n",
    "6. we add each sample to the channel\n",
    "7. we add each channel to the measurement, and go back to 1. until all channels were considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e486658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in regions:\n",
    "    chan = ROOT.RooStats.HistFactory.Channel(region)\n",
    "\n",
    "    # add data\n",
    "    chan.SetData(f'{region}_{k_data}{k_suffix}', # histo name, histo file, histo path\n",
    "                 fnames[region],\n",
    "                 f'{region}/{k_data}/{k_nominal}')\n",
    "    \n",
    "    chan.SetStatErrorConfig(0.05, 'Poisson')\n",
    "\n",
    "    ########\n",
    "    # add signal\n",
    "    sname = 'ttH'\n",
    "    sample_ttH = ROOT.RooStats.HistFactory.Sample(sname, # name, histo name, histo file, histo path\n",
    "                                              f'{region}_{sname}{k_suffix}',\n",
    "                                              fnames[region],\n",
    "                                              f'{region}/{sname}/{k_nominal}')\n",
    "    sample_ttH.SetNormalizeByTheory(False) # we use our own NP for luminosity\n",
    "    \n",
    "    for sysname, sysvalue in norm_systs[region][sname].items():\n",
    "        sample_ttH.AddOverallSys(sysname, sysvalue[0], sysvalue[1]) # name, -1sigma effect, +1sigma effect\n",
    "    sample_ttH.AddNormFactor('mu_ttH', 0, -10, 20) # NB default value is zero\n",
    "    sample_ttH.GetStatError().Activate(False)\n",
    "    chan.AddSample(sample_ttH)\n",
    "    \n",
    "    ########\n",
    "    # add ttbar\n",
    "    sname = 'ttbar'\n",
    "    sample_tt = ROOT.RooStats.HistFactory.Sample(sname, # name, histo name, histo file, histo path\n",
    "                                              f'{region}_{sname}{k_suffix}',\n",
    "                                              fnames[region],\n",
    "                                              f'{region}/{sname}/{k_nominal}')\n",
    "    sample_tt.SetNormalizeByTheory(False) # we use our own NP for luminosity\n",
    "    \n",
    "    for sysname, sysvalue in norm_systs[region][sname].items():\n",
    "        sample_tt.AddOverallSys(sysname, sysvalue[0], sysvalue[1]) # name, -1sigma effect, +1sigma effect\n",
    "    \n",
    "    syst_ = 'JES_Scenario1_NP1'\n",
    "    sample_tt.AddHistoSys(syst_, # name, histo name -1sigma, histo file -1sigma, histo path -1sigma, histo name +1sigma, histo file +1sigma, histo path +1sigma\n",
    "                          f'{region}_{sname}_{syst_}_Shape_Down{k_suffix}', fnames[region], f'{region}/{sname}/{syst_}',\n",
    "                          f'{region}_{sname}_{syst_}_Shape_Up{k_suffix}', fnames[region], f'{region}/{sname}/{syst_}',\n",
    "                         ) \n",
    "    sample_tt.AddNormFactor('mu_tt', 1, 0, 20)\n",
    "    sample_tt.GetStatError().Activate(False)\n",
    "    chan.AddSample(sample_tt)\n",
    "    \n",
    "    ########\n",
    "    # add singleTop\n",
    "    sname = 'singleTop'\n",
    "    sample_st = ROOT.RooStats.HistFactory.Sample(sname, # name, histo name, histo file, histo path\n",
    "                                              f'{region}_{sname}{k_suffix}',\n",
    "                                              fnames[region],\n",
    "                                              f'{region}/{sname}/{k_nominal}')\n",
    "    sample_st.SetNormalizeByTheory(False) # we use our own NP for luminosity\n",
    "    \n",
    "    for sysname, sysvalue in norm_systs[region][sname].items():\n",
    "        sample_st.AddOverallSys(sysname, sysvalue[0], sysvalue[1]) # name, -1sigma effect, +1sigma effect\n",
    "    \n",
    "    sample_st.GetStatError().Activate(False)\n",
    "    chan.AddSample(sample_st)\n",
    "\n",
    "\n",
    "    \n",
    "    # finally, collect the channel\n",
    "    meas.AddChannel(chan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f224bc",
   "metadata": {},
   "source": [
    "Eventually, we ask HistFactory to actually go and check the histograms, do its magic and create The Likelihood Model. We also persist this likelihood model in XML format, for our afternoons of debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cb6d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "meas.CollectHistograms()\n",
    "meas.PrintTree()\n",
    "meas.PrintXML('../xml', meas.GetOutputFilePrefix())\n",
    "\n",
    "ROOT.RooStats.HistFactory.MakeModelAndMeasurementFast(meas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76939df",
   "metadata": {},
   "source": [
    "And that's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c069977",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bcb042",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../xml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5e1a52",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Let's try to add more systematics (all those we listed before and for which we have histogram inputs in the root files).\n",
    "Specifically, we want to have all these systematics added: `'BTag_B_NP1', 'BTag_B_NP2', 'BTag_B_NP3', 'JES_Scenario1_NP2', 'BTag_C_NP1', 'tt_Shower', 'JER'` (on top of `'JES_Scenario1_NP1'` that we added already).\n",
    "All of these have both a \"normalization\" and \"shape\" effects.\n",
    "For the normalization effects, we need to add by hand the numbers, as we did for `lumi`, `JES_Scenario1_NP1` and the cross-section uncertainties.\n",
    "\n",
    "These are the numbers to add:\n",
    "```\n",
    "Region ljets_HThad_5j3b:\n",
    "ttH:\n",
    "  \"BTag_B_NP1\"  High=\"0.8697\"  Low=\"1.1303\"\n",
    "  \"BTag_B_NP2\"  High=\"1.02549\"  Low=\"0.974511\"\n",
    "  \"BTag_B_NP3\"  High=\"0.982417\"  Low=\"1.01758\"\n",
    "ttbar:\n",
    "  \"BTag_B_NP1\"  High=\"0.87674\"  Low=\"1.12326\"\n",
    "  \"BTag_B_NP2\"  High=\"1.02837\"  Low=\"0.971628\"\n",
    "  \"BTag_B_NP3\"  High=\"0.986622\"  Low=\"1.01338\"\n",
    "  \"BTag_C_NP1\"  High=\"0.916677\"  Low=\"1.08332\"\n",
    "  \"tt_Shower\"  High=\"1.06324\"  Low=\"0.936763\"\n",
    "singleTop:\n",
    "  \"BTag_B_NP1\"  High=\"0.874054\"  Low=\"1.12595\"\n",
    "  \"BTag_B_NP2\"  High=\"1.0217\"  Low=\"0.978299\"\n",
    "  \"BTag_B_NP3\"  High=\"0.98987\"  Low=\"1.01013\"\n",
    "  \"BTag_C_NP1\"  High=\"0.920117\"  Low=\"1.07988\"\n",
    "  \"JES_Scenario1_NP2\"  High=\"1.034\"  Low=\"0.965996\"\n",
    "  \n",
    "Region ljets_Mbb_ge6jge4b:\n",
    "ttH:\n",
    "  \"BTag_B_NP1\"  High=\"0.780949\"  Low=\"1.21905\"\n",
    "  \"BTag_B_NP2\"  High=\"1.04522\"  Low=\"0.954776\"\n",
    "  \"BTag_B_NP3\"  High=\"0.976712\"  Low=\"1.02329\"\n",
    "ttbar:\n",
    "  \"BTag_B_NP1\"  High=\"0.874133\"  Low=\"1.12587\"\n",
    "  \"BTag_B_NP2\"  High=\"1.02803\"  Low=\"0.971967\"\n",
    "  \"BTag_B_NP3\"  High=\"0.986524\"  Low=\"1.01348\"\n",
    "  \"BTag_C_NP1\"  High=\"0.915986\"  Low=\"1.08401\"\n",
    "  \"tt_Shower\"  High=\"1.06498\"  Low=\"0.935018\"\n",
    "singleTop:\n",
    "  \"BTag_B_NP1\"  High=\"0.870235\"  Low=\"1.12977\"\n",
    "  \"BTag_B_NP2\"  High=\"1.02002\"  Low=\"0.97998\"\n",
    "  \"BTag_B_NP3\"  High=\"0.989511\"  Low=\"1.01049\"\n",
    "  \"BTag_C_NP1\"  High=\"0.920832\"  Low=\"1.07917\"\n",
    "  \"JES_Scenario1_NP2\"  High=\"1.03127\"  Low=\"0.968725\"\n",
    "```\n",
    "(where nothing is listed, the normalization effect is zero).\n",
    "\n",
    "Then, for the shape part, we need to replicate these lines:\n",
    "```\n",
    "    syst_ = 'JES_Scenario1_NP1'\n",
    "    sample_tt.AddHistoSys(syst_, # name, histo name -1sigma, histo file -1sigma, histo path -1sigma, histo name +1sigma, histo file +1sigma, histo path +1sigma\n",
    "                          f'{region}_{sname}_{syst_}_Shape_Down{k_suffix}', fnames[region], f'{region}/{sname}/{syst_}',\n",
    "                          f'{region}_{sname}_{syst_}_Shape_Up{k_suffix}', fnames[region], f'{region}/{sname}/{syst_}',\n",
    "                         )\n",
    "```\n",
    "e.g. by replacing them with something like this (in the case of the ttbar background):\n",
    "```\n",
    "    shapeSysts = ['BTag_B_NP1','BTag_B_NP2','BTag_B_NP3','BTag_C_NP1','tt_Shower','JER']\n",
    "    for syst_ in shapeSysts:\n",
    "        sample_tt.AddHistoSys(syst_, \n",
    "                              f'{region}_{sname}_{syst_}_Shape_Down{k_suffix}', fnames[region], f'{region}/{sname}/{syst_}',\n",
    "                              f'{region}_{sname}_{syst_}_Shape_Up{k_suffix}', fnames[region], f'{region}/{sname}/{syst_}',\n",
    "                             )\n",
    "```\n",
    "For the other samples, the lists of shape systematics are the following:\n",
    "```\n",
    "ttH: ['BTag_B_NP1','BTag_B_NP2','BTag_B_NP3','BTag_C_NP1']\n",
    "singleTop: none\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b57b83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
